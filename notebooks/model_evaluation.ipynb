{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import cairosvg\n",
    "from IPython.core.display import display, Markdown\n",
    "from PIL import Image\n",
    "from json_dict import JsonDict\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "import main\n",
    "import pandas as pd\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "from rdkit import Chem\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "os.chdir(os.path.dirname(main.__file__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "FORCE_NEW=False\n",
    "SHOW_IMAGES=False\n",
    "SEED=1995\n",
    "DPI=300\n",
    "#EVALUATION_PARAMETER\n",
    "c_min=0\n",
    "c_max=30\n",
    "deg_min=10\n",
    "deg_max=300\n",
    "\n",
    "pixel=100*100\n",
    "\n",
    "true_name=\"z_average\"\n",
    "predicted_name=\"predicted_z_average\"\n",
    "metric_steps=[\"training\",\"validation\"]\n",
    "metric_epoch_limit=2000\n",
    "metric_highlighted_epochs=[10,50,99,500]\n",
    "\n",
    "display_quantiles = [1.,0.99,0.95,0.90]\n",
    "\n",
    "random_comparison_true_pred_number=40"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "#PRECALCULAIONS\n",
    "pixerl_1d=np.sqrt(pixel)\n",
    "c_delt=(c_max-c_min)/pixerl_1d\n",
    "deg_delt=(deg_max-deg_min)/(pixerl_1d-1)\n",
    "\n",
    "c_delt =(c_max-c_min)/(pixerl_1d-1)\n",
    "\n",
    "concentrations = np.arange(0,30,0.1)\n",
    "poly_deg = np.arange(deg_min,deg_max+deg_delt,deg_delt)\n",
    "concentrations = np.arange(c_min,c_max+c_delt,c_delt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#FUNCTIONS\n",
    "def load_config(model_path):\n",
    "    if not os.path.exists(os.path.join(model_path,\"config.json\")):\n",
    "        raise FileNotFoundError(\"Model config '{}' not found\".format(os.path.join(model_path,\"config.json\")))\n",
    "    return JsonDict(os.path.join(model_path,\"config.json\"))\n",
    "\n",
    "def load_model(config):\n",
    "    model = main.load_model(config.getsubdict(\"model\"))\n",
    "    return model\n",
    "\n",
    "def load_data(config):\n",
    "    training_history=config.get(\"training\",\"training_history\",default=[])\n",
    "    if len(training_history)<1:\n",
    "        raise ValueError(\"model '{}' has no training history\".format(config.get(\"model\",\"name\")))\n",
    "    training_data = training_history[-1]['args']['train']\n",
    "    if not os.path.exists(training_data):\n",
    "        raise FileNotFoundError(\"Training data '{}' not found\".format(training_data))\n",
    "\n",
    "    train_df = pd.read_csv(training_data)\n",
    "\n",
    "    test_data = training_data.replace(\"train_data_size_kept\",\"train_data_size_removed\")\n",
    "    if not os.path.exists(test_data):\n",
    "            raise FileNotFoundError(\"Training data '{}' not found\".format(test_data))\n",
    "    test_df = pd.read_csv(test_data)\n",
    "    return train_df,test_df\n",
    "\n",
    "def data_prediction(model,train_df,test_df,eval_dir):\n",
    "    def _pred(df):\n",
    "        if np.any(np.isnan(df[\"dp\"])):\n",
    "            raise ValueError(\"DP is None in for data with '{}'\".format(model.name) )\n",
    "        df =  main.predict(model,df)\n",
    "        df[\"error\"] = np.abs(df[predicted_name]-df[true_name])\n",
    "        df[\"rel_error\"] = df[\"error\"]/df[true_name]\n",
    "        return df\n",
    "\n",
    "    new=False\n",
    "    n_train_df=None\n",
    "    if os.path.exists(os.path.join(eval_dir,\"train_df.csv\")):\n",
    "        n_train_df = pd.read_csv(os.path.join(eval_dir,\"train_df.csv\"))\n",
    "\n",
    "    if n_train_df is None or n_train_df.shape[0] != train_df.shape[0] or \\\n",
    "                not n_train_df[['circular_smiles_10']].equals(train_df[['circular_smiles_10']]) or \\\n",
    "                not np.allclose(n_train_df[['dp','polymer_concentration',\"pdi\",\"with_pva\",\"z_average\"]].values,\n",
    "                                                train_df[['dp','polymer_concentration',\"pdi\",\"with_pva\",\"z_average\"]].values\n",
    "                                            ) or \\\n",
    "                 not np.allclose(main.predict(model,train_df.iloc[[0]],verbose=False)[predicted_name].values,\n",
    "                                 n_train_df.iloc[[0]][predicted_name].values\n",
    "                                 ):\n",
    "        n_train_df= _pred(train_df)\n",
    "        n_train_df.to_csv(os.path.join(eval_dir,\"train_df.csv\"),index=None)\n",
    "        new=True\n",
    "\n",
    "    n_test_df=None\n",
    "    if os.path.exists(os.path.join(eval_dir,\"test_df.csv\")):\n",
    "        n_test_df = pd.read_csv(os.path.join(eval_dir,\"test_df.csv\"))\n",
    "\n",
    "    if n_test_df is None or n_test_df.shape[0] != test_df.shape[0] or \\\n",
    "                not n_test_df[['circular_smiles_10']].equals(test_df[['circular_smiles_10']]) or \\\n",
    "                not np.allclose(n_test_df[['dp','polymer_concentration',\"pdi\",\"with_pva\",\"z_average\"]].values,\n",
    "                                                test_df[['dp','polymer_concentration',\"pdi\",\"with_pva\",\"z_average\"]].values\n",
    "                                            ) or \\\n",
    "            not np.allclose(main.predict(model,test_df.iloc[[0]],verbose=False)[predicted_name].values,\n",
    "                            n_test_df.iloc[[0]][predicted_name].values\n",
    "                            ):\n",
    "        n_test_df= _pred(test_df)\n",
    "        n_test_df.to_csv(os.path.join(eval_dir,\"test_df.csv\"),index=None)\n",
    "        new=True\n",
    "\n",
    "    return n_train_df,n_test_df,new\n",
    "\n",
    "def display_metrics(model,eval_dir):\n",
    "    td = pickle.load(open(os.path.join(model.get_dir(),\"training\",\"training_metrics.mtr\"), \"rb\"))\n",
    "    steps=[]\n",
    "    epochs=[]\n",
    "    metrics=[]\n",
    "    for step, data in td.items():\n",
    "        if step == \"epochs\":\n",
    "            continue\n",
    "        steps.append(step)\n",
    "        for epoch, dmetrics in data.items():\n",
    "            if epoch not in epochs:\n",
    "                epochs.append(epoch)\n",
    "            for metric,mdata in dmetrics.items():\n",
    "                if metric not in metrics:\n",
    "                    metrics.append(metric)\n",
    "    epochs=np.sort(epochs)\n",
    "    steps=[s for s in steps if s in metric_steps]\n",
    "\n",
    "    data=np.zeros((len(steps),len(metrics),len(epochs)))*np.nan\n",
    "    for i,step in enumerate(steps):\n",
    "                for j,metric in enumerate(metrics):\n",
    "                    for k,epoch in enumerate(epochs):\n",
    "                        try:\n",
    "                            data[i,j,k] = td[step][epoch][metric]\n",
    "                        except KeyError as ke:\n",
    "                            raise ke\n",
    "\n",
    "\n",
    "    def _disp(from_epoch,to_epoch,name,highlights=[]):\n",
    "        b=iter([\"-\",\"--\",'-.',':'])\n",
    "        plt.figure(dpi=300,figsize=(12,3))\n",
    "\n",
    "        for i,step in enumerate(steps):\n",
    "            for j,metric in enumerate(metrics):\n",
    "                plt.plot(epochs[from_epoch:to_epoch],data[i,j,from_epoch:to_epoch],next(b),label=step.capitalize()+\" \"+metric)\n",
    "\n",
    "        for h in highlights:\n",
    "            if h>=from_epoch and h<=to_epoch:\n",
    "                plt.axvline(x=h, lw=3, alpha=0.5)\n",
    "\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.title(\"Loss\")\n",
    "        plt.ylabel(\"Loss value\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.savefig(os.path.join(eval_dir,name+\".png\"),dpi=DPI)\n",
    "        if SHOW_IMAGES:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    # w/o highlights\n",
    "    # full epoch\n",
    "    _disp( from_epoch = 0,to_epoch = len(epochs),name=\"full_training_metrics\")\n",
    "    # to fixed limit epoch\n",
    "    _disp( from_epoch = 0,to_epoch = min(metric_epoch_limit,len(epochs)),name=\"fixed_training_metrics\")\n",
    "\n",
    "    #w highlights\n",
    "    # full epoch\n",
    "    _disp( from_epoch = 0,to_epoch = len(epochs),highlights=metric_highlighted_epochs,name=\"full_training_metrics_hl\")\n",
    "    # to fixed limit epoch\n",
    "    _disp( from_epoch = 0,to_epoch = min(metric_epoch_limit,len(epochs)),highlights=metric_highlighted_epochs,name=\"fixed_training_metrics_hl\")\n",
    "\n",
    "def calc_metrics( train_df,test_df):\n",
    "    comb_df=pd.concat([train_df,test_df])\n",
    "    d={\n",
    "              \"training_relmae\":np.abs(train_df[\"rel_error\"]).mean(),\n",
    "              \"training_rmse\":np.sqrt((train_df[\"error\"]**2).mean()),\n",
    "              \"test_relmae\":np.abs(test_df[\"rel_error\"]).mean(),\n",
    "              \"test_rmse\":np.sqrt((test_df[\"error\"]**2).mean()),\n",
    "                \"combined_relmae\":np.abs(comb_df[\"rel_error\"]).mean(),\n",
    "                \"combined_rmse\":np.sqrt((comb_df[\"error\"]**2).mean()),\n",
    "          }\n",
    "\n",
    "    s=\"Metrics:<br>\"\n",
    "    s+=\"--training data:<br>\"\n",
    "    s+=\"----RELMAE: {}<br>\".format(d[\"training_relmae\"])\n",
    "    s+=\"----RMSE: {}<br>\".format(d[\"training_rmse\"])\n",
    "    s+=\"--test data:<br>\"\n",
    "    s+=\"----RELMAE: {}<br>\".format(d[\"test_relmae\"])\n",
    "    s+=\"----RMSE: {}<br>\".format(d[\"test_rmse\"])\n",
    "    s+=\"--combined data:<br>\"\n",
    "    s+=\"----RELMAE: {}<br>\".format(d[\"combined_relmae\"])\n",
    "    s+=\"----RMSE: {}<br>\".format(d[\"combined_rmse\"])\n",
    "    display(Markdown(s))\n",
    "    return d\n",
    "\n",
    "\n",
    "def highlighted_training_steps(model,highlighted,eval_dir):\n",
    "    import json\n",
    "    with open(os.path.join(model.get_dir(),\"training\",\"ValidationPointRecorderCallback.lst\"), \"r\") as f:\n",
    "        dlist=json.loads(f.read())\n",
    "    available_epochs=[]\n",
    "    for e in range(len(dlist)):\n",
    "        if len(dlist[e][0]) > 0:\n",
    "            available_epochs.append(e)\n",
    "    for h in highlighted:\n",
    "        e = available_epochs[np.argmin(np.abs(np.array(available_epochs)-h))]\n",
    "        true,pred = dlist[e]\n",
    "        true,pred = np.array(true),np.array(pred)\n",
    "        _min = true.min()\n",
    "        _max =true.max()\n",
    "        over=0.05\n",
    "        _min,_max = _min - over*(_max-_min) , _max + over*(_max-_min)\n",
    "\n",
    "        plt.figure(dpi=100)\n",
    "        plt.plot([_min,_max],[_min,_max],\"--\",alpha=0.5)\n",
    "\n",
    "        plt.plot(true,pred,\"bo\",alpha=0.7,\n",
    "                 markeredgecolor=(1,1,1),markersize=10,label=\"epoch {}\".format(e))\n",
    "        plt.xlabel(\"Measured\")\n",
    "        plt.ylabel(\"Predicted\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.savefig(os.path.join(eval_dir,\"highlighted_training_steps_{}.png\".format(h)),dpi=DPI)\n",
    "        if SHOW_IMAGES:\n",
    "           plt.show()\n",
    "        plt.close()\n",
    "\n",
    "def get_mols(train_df,test_df,eval_dir):\n",
    "    unique_s=list(train_df[\"circular_smiles_10\"].unique())\n",
    "    all_smiles = unique_s\n",
    "\n",
    "    train_mols=Chem.Draw.MolsToGridImage([Chem.MolFromSmiles(smiles) for smiles in unique_s],molsPerRow=min(len(unique_s),5),subImgSize=(DPI,DPI))\n",
    "    train_mols.save(os.path.join(eval_dir,\"mols_train.png\"))\n",
    "    if SHOW_IMAGES:\n",
    "        display(train_mols)\n",
    "\n",
    "    unique_s = list(test_df[\"circular_smiles_10\"].unique())\n",
    "    all_smiles.extend(unique_s)\n",
    "\n",
    "    test_mols = Chem.Draw.MolsToGridImage([Chem.MolFromSmiles(smiles) for smiles in unique_s],molsPerRow=min(len(unique_s),5),subImgSize=(DPI,DPI))\n",
    "    test_mols.save(os.path.join(eval_dir,\"mols_test.png\"))\n",
    "    if SHOW_IMAGES:\n",
    "            display(test_mols)\n",
    "\n",
    "    return {smiles:Chem.MolFromSmiles(smiles) for smiles in all_smiles}\n",
    "\n",
    "def true_vs_predicted_by_mol(train_df,test_df,eval_dir):\n",
    "    _min = min(train_df[true_name].min(),test_df[true_name].min())\n",
    "    _max = max(train_df[true_name].max(),test_df[true_name].max())\n",
    "    over=0.05\n",
    "    _min,_max = _min - over*(_max-_min) , _max + over*(_max-_min)\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot([_min,_max],[_min,_max],\"--\")\n",
    "    for su in train_df.smiles_ru.unique():\n",
    "        sd = train_df[train_df.smiles_ru == su]\n",
    "        plt.plot(sd[true_name],sd[predicted_name],\"o\",label=su,alpha=0.7,\n",
    "                 markeredgecolor=\"w\",\n",
    "                 markersize=5\n",
    "                )\n",
    "    plt.xlabel(\"Measured Z-Average [nm]\")\n",
    "    plt.ylabel(\"Predicted Z-Average [nm]\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(eval_dir,\"true_vs_false_mol_train.png\"),dpi=DPI)\n",
    "    if SHOW_IMAGES:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot([_min,_max],[_min,_max],\"--\")\n",
    "    for su in test_df.smiles_ru.unique():\n",
    "        sd = test_df[test_df.smiles_ru == su]\n",
    "        plt.plot(sd[true_name],sd[predicted_name],\"o\",label=su,alpha=0.7,\n",
    "                 markeredgecolor=\"b\",\n",
    "                 markersize=5\n",
    "                )\n",
    "    plt.xlabel(\"Measured Z-Average [nm]\")\n",
    "    plt.ylabel(\"Predicted Z-Average [nm]\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(eval_dir,\"true_vs_false_mol_test.png\"),dpi=DPI)\n",
    "    if SHOW_IMAGES:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot([_min,_max],[_min,_max],\"--\")\n",
    "    for su in train_df.smiles_ru.unique():\n",
    "        sd = train_df[train_df.smiles_ru == su]\n",
    "        plt.plot(sd[true_name],sd[predicted_name],\"o\",label=su,alpha=0.7,\n",
    "                 markeredgecolor=\"w\",\n",
    "                 markersize=5\n",
    "                )\n",
    "    for su in test_df.smiles_ru.unique():\n",
    "            sd = test_df[test_df.smiles_ru == su]\n",
    "            plt.plot(sd[true_name],sd[predicted_name],\"o\",label=su,alpha=0.7,\n",
    "                     markeredgecolor=\"b\",\n",
    "                     markersize=5\n",
    "                    )\n",
    "    plt.xlabel(\"Measured Z-Average [nm]\")\n",
    "    plt.ylabel(\"Predicted Z-Average [nm]\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(eval_dir,\"true_vs_false_mol_both.png\"),dpi=DPI)\n",
    "    if SHOW_IMAGES:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def true_vs_predicted_quantilized(train_df,test_df,eval_dir):\n",
    "    _min = min(train_df[true_name].min(),test_df[true_name].min())\n",
    "    _max = max(train_df[true_name].max(),test_df[true_name].max())\n",
    "    over=0.05\n",
    "    _min,_max = _min - over*(_max-_min) , _max + over*(_max-_min)\n",
    "    qs=list(sorted(display_quantiles))\n",
    "\n",
    "    markercolors=[\"w\",\"k\",\"r\",\"b\"]\n",
    "\n",
    "    def _plot(df,name):\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.plot([_min,_max],[_min,_max],\"--\")\n",
    "        dfs=[]\n",
    "        if isinstance(df,(list,tuple)):\n",
    "            assert isinstance(name,(list,tuple)),\"name not a list or tuble but df is\"\n",
    "            if len(df)>1:\n",
    "                df,dfs = df[0],df[1:]\n",
    "                name,names=name[0],name[1:]\n",
    "            else:\n",
    "                df,dfs=df[0],[]\n",
    "                name,names=name[0],[]\n",
    "\n",
    "        sorted_indices = np.argsort(df[\"error\"])\n",
    "\n",
    "\n",
    "\n",
    "        for i,q in enumerate(qs):\n",
    "                if i==0:\n",
    "                    s=0\n",
    "                else:\n",
    "                    s=int(len(sorted_indices)*qs[i-1])\n",
    "                e=int(q*len(sorted_indices))\n",
    "                plt.plot(df[true_name].iloc[sorted_indices[s:e]],\n",
    "                         df[predicted_name].iloc[sorted_indices[s:e]],\n",
    "                         \"o\",\n",
    "                         label=r\"{} quantile ($\\overline{{error}}={:.1f}\\pm{:.1f}$%)\".format(q,\n",
    "                                                                                                  100*(df[\"rel_error\"]).iloc[sorted_indices[:e]].mean(),\n",
    "                                                                                                 100*(df[\"rel_error\"]).iloc[sorted_indices[:e]].std()\n",
    "                                                                                                 ),\n",
    "                         alpha=0.7,\n",
    "                         markeredgecolor=markercolors[0],\n",
    "                         markersize=4/q**2,\n",
    "                        )\n",
    "        for i,sdf in enumerate(dfs):\n",
    "            plt.plot(sdf[true_name],\n",
    "                         sdf[predicted_name],\n",
    "                         \"o\",\n",
    "                         label=r\"{} data ($\\overline{{error}}={:.1f}\\pm{:.1f}$%)\".format(names[i],(100*sdf[\"rel_error\"]).mean(),\n",
    "                                                                                        100*(sdf[\"rel_error\"]).std()\n",
    "                                                                                        ),\n",
    "                         alpha=0.7,\n",
    "                         markeredgecolor=markercolors[i+1],\n",
    "                         markersize=4\n",
    "                        )\n",
    "        plt.xlabel(\"Measured Z-Average [nm]\")\n",
    "        plt.ylabel(\"Predicted Z-Average [nm]\")\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(eval_dir,\"true_vs_predicted_quantilized_{}.png\".format(name)),dpi=DPI)\n",
    "        if SHOW_IMAGES:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    _plot(train_df,name=\"train\")\n",
    "    _plot(test_df,name=\"test\")\n",
    "    _plot(pd.concat([train_df,test_df]),name=\"both\")\n",
    "    _plot([train_df,test_df],name=[\"train_super_test\",\"test\"])\n",
    "\n",
    "def random_comparison_true_pred(train_df,test_df,eval_dir):\n",
    "    def _plot(df,name):\n",
    "        true=df[true_name].values\n",
    "        pred=df[predicted_name].values\n",
    "\n",
    "        idx  = np.arange(len(true))\n",
    "        np.random.seed(SEED)\n",
    "        idx=np.random.choice(idx,size=random_comparison_true_pred_number)\n",
    "        width=random_comparison_true_pred_number/100\n",
    "        prehw=plt.rcParams['hatch.linewidth']\n",
    "        plt.rcParams['hatch.linewidth']=width\n",
    "\n",
    "        fig,a = plt.subplots()\n",
    "        a.bar(np.arange(len(idx)) - width/2,true[idx],width,label=\"true\", hatch=\"xxxxxx\")\n",
    "        a.bar(np.arange(len(idx)) + width/2,pred[idx],width,label=\"predicted\", hatch=\"....\")\n",
    "        a.bar(np.arange(len(idx)),np.abs(true[idx]-pred[idx]),label=\"difference\")\n",
    "\n",
    "        plt.ylabel(\"Z-Average [nm]\")\n",
    "        plt.title(\"Random comparison of Z-Average predicted and true \")\n",
    "        a.axes.xaxis.set_visible(False)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.savefig(os.path.join(eval_dir,\"random_comparison_true_pred_{}.png\".format(name)),dpi=DPI)\n",
    "        if SHOW_IMAGES:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "        fig,a = plt.subplots()\n",
    "        a.bar(np.arange(len(idx)) - width/2,1,width,label=\"true\", hatch=\"xxxxxx\")\n",
    "        a.bar(np.arange(len(idx)) + width/2,1*pred[idx]/true[idx], width,label=\"predicted\", hatch=\"....\")\n",
    "        a.bar(np.arange(len(idx)), np.abs(1 - (1*pred[idx]/true[idx])) ,label=\"difference\")\n",
    "\n",
    "        plt.ylabel(\"rel. Z-Average \")\n",
    "        plt.title(\"Random comparison of Z-Average predicted and measured \")\n",
    "        a.axes.xaxis.set_visible(False)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.savefig(os.path.join(eval_dir,\"random_comparison_true_pred_{}_rel.png\".format(name)),dpi=DPI)\n",
    "        if SHOW_IMAGES:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "        plt.rcParams['hatch.linewidth']=prehw\n",
    "\n",
    "    _plot(train_df,name=\"train\")\n",
    "    _plot(test_df,name=\"test\")\n",
    "    _plot(pd.concat([train_df,test_df]),name=\"both\")\n",
    "\n",
    "def calculate_prediction_grid(model,all_mols,eval_dir):\n",
    "    #full_df=pd.DataFrame(columns=['dp','polymer_concentration',\"pdi\",\"with_pva\",'circular_smiles_10'])\n",
    "    df_data=[]\n",
    "    for smiles in all_mols.keys():\n",
    "        for conc in concentrations:\n",
    "            for deg in poly_deg:\n",
    "                for wpva in [0,1]:\n",
    "                    df_data.append({\n",
    "                        'dp' : deg,\n",
    "                        'polymer_concentration' : conc,\n",
    "                        \"pdi\":1,\n",
    "                        \"with_pva\":wpva,\n",
    "                        'circular_smiles_10':smiles\n",
    "                    })\n",
    "\n",
    "    pred_df=None\n",
    "    new=False\n",
    "    if os.path.exists(os.path.join(eval_dir,\"prediction_grid.csv\")):\n",
    "        pred_df=pd.read_csv(os.path.join(eval_dir,\"prediction_grid.csv\"))\n",
    "    full_df = pd.DataFrame(df_data)\n",
    "\n",
    "    if pred_df is None or pred_df.shape[0] != full_df.shape[0] or \\\n",
    "            not pred_df[['circular_smiles_10']].equals(full_df[['circular_smiles_10']]) or \\\n",
    "            not np.allclose(pred_df[['dp','polymer_concentration',\"pdi\",\"with_pva\"]].values,\n",
    "                                            full_df[['dp','polymer_concentration',\"pdi\",\"with_pva\"]].values\n",
    "                                        ) or \\\n",
    "             not np.allclose(main.predict(model,full_df.iloc[[0]],verbose=False)[predicted_name].values,\n",
    "                             pred_df.iloc[[0]][predicted_name].values\n",
    "                             ):\n",
    "        pred_df = main.predict(model,full_df)\n",
    "        pred_df.to_csv(os.path.join(eval_dir,\"prediction_grid.csv\"),index=None)\n",
    "        new=True\n",
    "    return pred_df,new\n",
    "\n",
    "def plot_prediction_grid(prediction_df,train_df,test_df,eval_dir,max_h=None,min_h=0):\n",
    "    if max_h is None:\n",
    "        max_h=max(train_df[true_name].max(),test_df[true_name].max())\n",
    "    z_max=max_h\n",
    "    z_min=min_h\n",
    "\n",
    "    #import cairosvg\n",
    "    #from PIL import Image\n",
    "    #import os\n",
    "    def to_rdk_transparent_png(mol ):\n",
    "        dr = Chem.Draw.rdMolDraw2D.MolDraw2DSVG( DPI, DPI )\n",
    "        #dr.SetFontSize( 0.27 )\n",
    "        op = dr.drawOptions()\n",
    "        #for i in range( mol.GetNumAtoms() ) :\n",
    "        #    op.atomLabels[i]=mol.GetAtomWithIdx(i).GetSymbol()+str((i+1))\n",
    "        Chem.AllChem.Compute2DCoords( mol )\n",
    "        dr.DrawMolecule( mol )\n",
    "        dr.FinishDrawing()\n",
    "\n",
    "        svg = dr.GetDrawingText()\n",
    "        svg =  svg.replace(\"<rect style='opacity:1.0;fill:#\",\"<rect style='opacity:0.0;fill:#\")\n",
    "\n",
    "        cairosvg.svg2png(svg,write_to =\"temp.png\",dpi=DPI)\n",
    "        img = Image.open(\"temp.png\")\n",
    "        return img\n",
    "\n",
    "    for (smiles,with_pva), smiles_df in prediction_df.groupby([\"circular_smiles_10\",\"with_pva\"]):\n",
    "        sd = train_df[(train_df[\"circular_smiles_10\"]==smiles)]\n",
    "        if len(sd.dp.unique()) >0:\n",
    "            pass\n",
    "        else:\n",
    "            sd = test_df[(test_df[\"circular_smiles_10\"]==smiles)]\n",
    "\n",
    "        plt.figure(dpi=150)\n",
    "        predicted_z_average = smiles_df[\"predicted_z_average\"].values.astype(float)\n",
    "        #print(predicted_z_average)\n",
    "        XY_pred = (predicted_z_average.reshape(len(concentrations),-1)).T\n",
    "        plt.imshow(XY_pred,\n",
    "                        #extent=[c_min,c_max,30,0],\n",
    "             #            yticklabels=yticks\n",
    "                   cmap='rocket',\n",
    "                        )\n",
    "        plt.clim(z_min,z_max)\n",
    "\n",
    "\n",
    "        sd = sd[sd[\"with_pva\"]==with_pva]\n",
    "        #sd = sd[sd[\"with_pva\"]==with_pva]\n",
    "        dps=[]\n",
    "        poly_conc=[]\n",
    "        z_average=[]\n",
    "        for (dp,polymer_concentration), point_group in sd.groupby([\"dp\",\"polymer_concentration\"]):\n",
    "            dps.append(dp)\n",
    "            poly_conc.append(polymer_concentration)\n",
    "            z_average.append(point_group[\"z_average\"].mean())\n",
    "        new_dps=[np.argmin(np.abs(poly_deg-deg)) for deg in dps]\n",
    "        new_conc=[np.argmin(np.abs(concentrations-conc)) for conc in poly_conc]\n",
    "\n",
    "        k=7\n",
    "        old_xticks = plt.xticks()\n",
    "        plt.xticks(np.linspace(0,old_xticks[0].max(),k), np.round(np.linspace(concentrations.min(),concentrations.max(),k)))\n",
    "\n",
    "        old_yticks = plt.yticks()\n",
    "        plt.yticks(np.linspace(0,len(poly_deg),k), np.linspace(poly_deg.min(),poly_deg.max(),k,dtype=int))\n",
    "\n",
    "        plt.scatter(new_conc,new_dps,c=z_average,edgecolors=\"w\")\n",
    "        plt.clim(z_min,z_max)\n",
    "\n",
    "        plt.xlabel(\"Polymer concentration [g/L]\")\n",
    "        plt.ylabel(\"Degree of polymerization\")\n",
    "\n",
    "        title=\"{} {} PVA\".format(sd.polymer.unique()[0], \"with\" if with_pva else \"without\",)\n",
    "        plt.title(title)\n",
    "\n",
    "        trans_mol=to_rdk_transparent_png(Chem.MolFromSmiles(smiles))\n",
    "        if SHOW_IMAGES:\n",
    "            display(trans_mol)\n",
    "        trans_mol.save(os.path.join(\n",
    "            eval_dir,\n",
    "            \"trans_mol_{}.png\".format(sd.polymer.unique()[0]).replace(\" \",\"_\")\n",
    "        ))\n",
    "        cbar = plt.colorbar()\n",
    "\n",
    "        cbar.ax.get_yaxis().labelpad = 15\n",
    "        cbar.ax.set_ylabel('Z-Average [nm]', rotation=270)\n",
    "\n",
    "        plt.savefig(\n",
    "            os.path.join(\n",
    "                eval_dir,\"prediction_grid_{}.png\".format(title.replace(\" \",\"_\"))\n",
    "            ),dpi=DPI\n",
    "        )\n",
    "        if SHOW_IMAGES:\n",
    "            plt.show()\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "        #xypred = prednp.reshape((d_res,c_res))\n",
    "\n",
    "def plot_prediction_grid_3d(prediction_df,train_df,test_df,eval_dir,max_h=None,min_h=0):\n",
    "    if max_h is None:\n",
    "            max_h=max(train_df[true_name].max(),test_df[true_name].max())\n",
    "    z_max=max_h\n",
    "    z_min=min_h\n",
    "\n",
    "    for (smiles,with_pva), smiles_df in prediction_df.groupby([\"circular_smiles_10\",\"with_pva\"]):\n",
    "        sd = train_df[(train_df[\"circular_smiles_10\"]==smiles)]\n",
    "        if len(sd.dp.unique()) >0:\n",
    "            pass\n",
    "        else:\n",
    "            sd = test_df[(test_df[\"circular_smiles_10\"]==smiles)]\n",
    "        title=\"{} {} PVA\".format(sd.polymer.unique()[0], \"with\" if with_pva else \"without\",)\n",
    "\n",
    "        fig = plt.figure(dpi=300)\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.view_init(elev=15., azim=210)\n",
    "        predicted_z_average = smiles_df[\"predicted_z_average\"].values.astype(float)\n",
    "        Z = (predicted_z_average.reshape(len(concentrations),-1)).T\n",
    "\n",
    "        norm=matplotlib.colors.Normalize(vmin=z_min, vmax=z_max)\n",
    "        X, Y = np.meshgrid(concentrations, poly_deg)\n",
    "        p = ax.plot_surface(X,Y,Z,\n",
    "                          linewidth=0.1,\n",
    "                        cmap='rocket',\n",
    "                            norm=norm\n",
    "                         )\n",
    "\n",
    "        ax.set_zlim(z_min,z_max)\n",
    "\n",
    "        m = cm.ScalarMappable(cmap=p.cmap,norm=norm)\n",
    "        #m.set_array(np.array([z_min,z_max]))\n",
    "        #m.to_rgba()\n",
    "        plt.colorbar(m)\n",
    "\n",
    "    #    cbar = fig.colorbar(p)\n",
    "    #    cbar.set_clim(z_min,z_max)\n",
    "\n",
    "\n",
    "        plt.savefig(\n",
    "                    os.path.join(\n",
    "                        eval_dir,\"prediction_grid_3d_{}.png\".format(title.replace(\" \",\"_\"))\n",
    "                    ),dpi=DPI\n",
    "                )\n",
    "        if SHOW_IMAGES:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "        #xypred = prednp.reshape((d_res,c_res))\n",
    "\n",
    "def merge_images(eval_dir):\n",
    "    top=\"fixed_training_metrics.png\"\n",
    "    second=\"true_vs_false_mol_both.png\"\n",
    "    third=\"true_vs_predicted_quantilized_train_super_test.png\"\n",
    "    bottom_left=\"random_comparison_true_pred_both_rel.png\"\n",
    "    bottom_right=\"random_comparison_true_pred_both.png\"\n",
    "\n",
    "    GRIDS_PER_ROW=4\n",
    "\n",
    "    top,second,third,bottom_left,bottom_right = (os.path.join(eval_dir,n) for n in [top,second,third,bottom_left,bottom_right])\n",
    "    top,second,third,bottom_left,bottom_right = (Image.open(n) for n in [top,second,third,bottom_left,bottom_right])\n",
    "    dist=150\n",
    "\n",
    "    full_height=np.sum(\n",
    "        [s.size[1]+dist for s in  [top,second,third]] +\n",
    "        [np.max([s.size[1]+dist for s in  [bottom_left,bottom_right]])]\n",
    "    )\n",
    "\n",
    "    full_width=np.max([s.size[0]+dist for s in  [top,second,third]]+\n",
    "                      [np.sum([s.size[0]+dist for s in  [bottom_left,bottom_right]])]\n",
    "                      )\n",
    "\n",
    "    new_img = Image.new('RGBA',(full_width,full_height))\n",
    "    x_offset=0\n",
    "    y_offset=0\n",
    "\n",
    "    new_img.paste(top,(x_offset,y_offset))\n",
    "    y_offset+=top.size[1]+dist\n",
    "\n",
    "    new_img.paste(second,(x_offset,y_offset))\n",
    "    y_offset+=second.size[1]+dist\n",
    "\n",
    "    new_img.paste(third,(x_offset,y_offset))\n",
    "    y_offset+=third.size[1]+dist\n",
    "\n",
    "    new_img.paste(bottom_left,(x_offset,y_offset))\n",
    "    x_offset+=bottom_left.size[0]+dist\n",
    "\n",
    "    new_img.paste(bottom_right,(x_offset,y_offset))\n",
    "\n",
    "    new_img.save(os.path.join(eval_dir,\"merge_images_1.png\"))\n",
    "\n",
    "    files = [os.path.join(eval_dir,\"prediction_grid\",s) for s in sorted(os.listdir(os.path.join(eval_dir,\"prediction_grid\"))) if s.startswith(\"prediction_grid\")]\n",
    "\n",
    "    size=np.array(Image.open(files[0]).size)+dist\n",
    "\n",
    "    line=0\n",
    "    row=0\n",
    "    new_img = Image.new('RGBA',(int(size[0]*GRIDS_PER_ROW),int(size[1]*np.ceil(len(files)/GRIDS_PER_ROW))))\n",
    "    for i,f in enumerate(files) :\n",
    "        new_img.paste(Image.open(f),(int(row*size[0]),int(line*size[1])))\n",
    "        row += 1\n",
    "\n",
    "        if row >= GRIDS_PER_ROW:\n",
    "            line +=1\n",
    "            row=0\n",
    "\n",
    "    new_img.save(os.path.join(eval_dir,\"merge_images_2.png\"))\n",
    "\n",
    "def crop_images(eval_dir):\n",
    "    for folder in [os.path.join(eval_dir,s) for s in os.listdir(eval_dir) if os.path.isdir(os.path.join(eval_dir,s))]:\n",
    "        crop_images(folder)\n",
    "\n",
    "    files = [os.path.join(eval_dir,s) for s in os.listdir(eval_dir) if s.endswith(\".png\")]\n",
    "    for file in files:\n",
    "        image=Image.open(file)\n",
    "        image.load()\n",
    "\n",
    "        image_data = np.asarray(image)\n",
    "        if image_data.shape[2]<4:\n",
    "            continue\n",
    "\n",
    "        alpha=image_data[:,:,3]\n",
    "        non_empty_columns = np.where(alpha.max(axis=0)>0)[0]\n",
    "        non_empty_rows = np.where(alpha.max(axis=1)>0)[0]\n",
    "\n",
    "        cropBox = (min(non_empty_rows), max(non_empty_rows), min(non_empty_columns), max(non_empty_columns))\n",
    "\n",
    "        image_data_new = image_data[cropBox[0]:cropBox[1]+1, cropBox[2]:cropBox[3]+1 , :]\n",
    "\n",
    "        new_image = Image.fromarray(image_data_new)\n",
    "        new_image.save(file)\n",
    "\n",
    "def evaluate(model_path):\n",
    "    config=load_config(model_path)\n",
    "    model = load_model(config)\n",
    "    eval_dir = os.path.join(model_path,\"evaluation\")\n",
    "    os.makedirs(eval_dir,exist_ok=True)\n",
    "    display(Markdown(\"Evaluate {}:\".format(model.name)))\n",
    "    display_metrics(model,eval_dir=eval_dir)\n",
    "    train_df,test_df = load_data(config)\n",
    "    train_df,test_df,new = data_prediction(model,train_df,test_df,eval_dir)\n",
    "\n",
    "    all_mols = get_mols(train_df,test_df,eval_dir)\n",
    "    metrics = calc_metrics( train_df,test_df)\n",
    "    if new or FORCE_NEW:\n",
    "        highlighted_training_steps(model,metric_highlighted_epochs,eval_dir=eval_dir)\n",
    "        true_vs_predicted_by_mol(train_df,test_df,eval_dir)\n",
    "        true_vs_predicted_quantilized(train_df,test_df,eval_dir)\n",
    "        random_comparison_true_pred(train_df,test_df,eval_dir)\n",
    "\n",
    "    prediction_grid,new2=calculate_prediction_grid(model,all_mols,eval_dir)\n",
    "\n",
    "    if new or new2 or FORCE_NEW:\n",
    "        os.makedirs(os.path.join(eval_dir,\"prediction_grid\"),exist_ok=True)\n",
    "        plot_prediction_grid(prediction_grid,train_df,test_df,os.path.join(eval_dir,\"prediction_grid\"))\n",
    "        os.makedirs(os.path.join(eval_dir,\"prediction_grid_3d\"),exist_ok=True)\n",
    "        plot_prediction_grid_3d(prediction_grid,train_df,test_df,os.path.join(eval_dir,\"prediction_grid_3d\"))\n",
    "\n",
    "        crop_images(eval_dir)\n",
    "        merge_images(eval_dir)\n",
    "\n",
    "    display(Markdown(\"<br>\"))\n",
    "    return {**{\"path\":model_path,\"name\":model.name},**metrics,**config['model']}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['np_model_0', 'np_model_1', 'np_model_2', 'np_model_3', 'np_model_4', 'np_model_5', 'np_model_6', 'np_model_7', 'np_model_8', 'np_model_9', 'np_model_10', 'np_model_11', 'np_model_12', 'np_model_13']\n",
      "try loading model from /home/julian/IdeaProjects/nanoparticle_size_prediction/pretrained/np_model_0/np_model_0.pth\n",
      "load successful\n",
      "try loading model from /home/julian/IdeaProjects/nanoparticle_size_prediction/pretrained/np_model_1/np_model_1.pth\n",
      "load successful\n",
      "try loading model from /home/julian/IdeaProjects/nanoparticle_size_prediction/pretrained/np_model_2/np_model_2.pth\n",
      "load successful\n",
      "try loading model from /home/julian/IdeaProjects/nanoparticle_size_prediction/pretrained/np_model_3/np_model_3.pth\n",
      "load successful\n",
      "try loading model from /home/julian/IdeaProjects/nanoparticle_size_prediction/pretrained/np_model_4/np_model_4.pth\n",
      "load successful\n",
      "try loading model from /home/julian/IdeaProjects/nanoparticle_size_prediction/pretrained/np_model_5/np_model_5.pth\n",
      "load successful\n",
      "try loading model from /home/julian/IdeaProjects/nanoparticle_size_prediction/pretrained/np_model_6/np_model_6.pth\n",
      "load successful\n",
      "try loading model from /home/julian/IdeaProjects/nanoparticle_size_prediction/pretrained/np_model_7/np_model_7.pth\n",
      "load successful\n",
      "try loading model from /home/julian/IdeaProjects/nanoparticle_size_prediction/pretrained/np_model_8/np_model_8.pth\n",
      "load successful\n",
      "try loading model from /home/julian/IdeaProjects/nanoparticle_size_prediction/pretrained/np_model_9/np_model_9.pth\n",
      "load successful\n",
      "try loading model from /home/julian/IdeaProjects/nanoparticle_size_prediction/pretrained/np_model_10/np_model_10.pth\n",
      "load successful\n",
      "try loading model from /home/julian/IdeaProjects/nanoparticle_size_prediction/pretrained/np_model_11/np_model_11.pth\n",
      "load successful\n",
      "try loading model from /home/julian/IdeaProjects/nanoparticle_size_prediction/pretrained/np_model_12/np_model_12.pth\n",
      "load successful\n",
      "try loading model from /home/julian/IdeaProjects/nanoparticle_size_prediction/pretrained/np_model_13/np_model_13.pth\n",
      "load successful\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Evaluate np_model_0:"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Metrics:<br>--training data:<br>----RELMAE: 0.046140234949364305<br>----RMSE: 18.45506851818752<br>--test data:<br>----RELMAE: 2.810303556680282<br>----RMSE: 936.2270936024785<br>--combined data:<br>----RELMAE: 0.5690368234207278<br>----RMSE: 407.5384776933218<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Evaluate np_model_1:"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Metrics:<br>--training data:<br>----RELMAE: 0.06134875465198188<br>----RMSE: 24.91373884810834<br>--test data:<br>----RELMAE: 0.13327268590828104<br>----RMSE: 28.749340177189584<br>--combined data:<br>----RELMAE: 0.06798512736141596<br>----RMSE: 25.29201841008432<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Evaluate np_model_2:"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Metrics:<br>--training data:<br>----RELMAE: 0.054157322646257736<br>----RMSE: 21.84681237121438<br>--test data:<br>----RELMAE: 0.16733514405219008<br>----RMSE: 39.32268437270043<br>--combined data:<br>----RELMAE: 0.06464048387373988<br>----RMSE: 24.00622951968969<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Evaluate np_model_3:"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Metrics:<br>--training data:<br>----RELMAE: 0.05408083061211714<br>----RMSE: 20.087497191816265<br>--test data:<br>----RELMAE: 0.23788293065803182<br>----RMSE: 58.60377886680879<br>--combined data:<br>----RELMAE: 0.07437960190325842<br>----RMSE: 27.170464499785417<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Evaluate np_model_4:"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Metrics:<br>--training data:<br>----RELMAE: 0.06766859936869236<br>----RMSE: 25.79735617117761<br>--test data:<br>----RELMAE: 0.11288806947812947<br>----RMSE: 50.12836016807005<br>--combined data:<br>----RELMAE: 0.07153488822735461<br>----RMSE: 28.69586911413653<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Evaluate np_model_5:"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Metrics:<br>--training data:<br>----RELMAE: 0.05348779110854386<br>----RMSE: 20.512828020785296<br>--test data:<br>----RELMAE: 0.1428582690481444<br>----RMSE: 49.69794479654046<br>--combined data:<br>----RELMAE: 0.05931422411992501<br>----RMSE: 23.54498009272203<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Evaluate np_model_6:"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Metrics:<br>--training data:<br>----RELMAE: 0.05376514037392872<br>----RMSE: 22.438987287734943<br>--test data:<br>----RELMAE: 0.33462363735619244<br>----RMSE: 73.17097381920186<br>--combined data:<br>----RELMAE: 0.07947965185395785<br>----RMSE: 30.783160721204396<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Evaluate np_model_7:"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Metrics:<br>--training data:<br>----RELMAE: 0.05815956724228309<br>----RMSE: 23.881685751994<br>--test data:<br>----RELMAE: 0.20886189606684127<br>----RMSE: 44.17065208962158<br>--combined data:<br>----RELMAE: 0.07319221849660311<br>----RMSE: 26.609429840159663<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Evaluate np_model_8:"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Metrics:<br>--training data:<br>----RELMAE: 0.04937116492179503<br>----RMSE: 18.893948095868353<br>--test data:<br>----RELMAE: 0.13152901777101933<br>----RMSE: 48.569254585935695<br>--combined data:<br>----RELMAE: 0.05630789848975589<br>----RMSE: 22.9349722473274<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Evaluate np_model_9:"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Metrics:<br>--training data:<br>----RELMAE: 0.04749936377305033<br>----RMSE: 16.29361988844087<br>--test data:<br>----RELMAE: 0.401554036950253<br>----RMSE: 85.65043567811041<br>--combined data:<br>----RELMAE: 0.07903255518534126<br>----RMSE: 29.919955454340567<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Evaluate np_model_10:"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Metrics:<br>--training data:<br>----RELMAE: 0.10331032882547049<br>----RMSE: 34.48422808594879<br>--test data:<br>----RELMAE: 0.29768767066809465<br>----RMSE: 77.03428933458903<br>--combined data:<br>----RELMAE: 0.1206221690323305<br>----RMSE: 40.14694079825211<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Evaluate np_model_11:"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Metrics:<br>--training data:<br>----RELMAE: 0.05800806009632727<br>----RMSE: 20.635627453881828<br>--test data:<br>----RELMAE: 0.2567698173849615<br>----RMSE: 79.08807592815813<br>--combined data:<br>----RELMAE: 0.07478986860270642<br>----RMSE: 30.298348844514763<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Evaluate np_model_12:"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Metrics:<br>--training data:<br>----RELMAE: 0.05749805507072053<br>----RMSE: 23.535618293713302<br>--test data:<br>----RELMAE: 0.23515880804437847<br>----RMSE: 50.38787249512301<br>--combined data:<br>----RELMAE: 0.07521982594091084<br>----RMSE: 27.421370307234184<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Evaluate np_model_13:"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Metrics:<br>--training data:<br>----RELMAE: 0.05543022864216529<br>----RMSE: 20.599389861327552<br>--test data:<br>----RELMAE: 0.7029578058796456<br>----RMSE: 147.52400420616524<br>--combined data:<br>----RELMAE: 0.11471579592040983<br>----RMSE: 48.76541411060645<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "<br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "                      path         name  training_relmae  training_rmse  \\\n0    pretrained/np_model_0   np_model_0         0.046140      18.455069   \n1    pretrained/np_model_1   np_model_1         0.061349      24.913739   \n2    pretrained/np_model_2   np_model_2         0.054157      21.846812   \n3    pretrained/np_model_3   np_model_3         0.054081      20.087497   \n4    pretrained/np_model_4   np_model_4         0.067669      25.797356   \n5    pretrained/np_model_5   np_model_5         0.053488      20.512828   \n6    pretrained/np_model_6   np_model_6         0.053765      22.438987   \n7    pretrained/np_model_7   np_model_7         0.058160      23.881686   \n8    pretrained/np_model_8   np_model_8         0.049371      18.893948   \n9    pretrained/np_model_9   np_model_9         0.047499      16.293620   \n10  pretrained/np_model_10  np_model_10         0.103310      34.484228   \n11  pretrained/np_model_11  np_model_11         0.058008      20.635627   \n12  pretrained/np_model_12  np_model_12         0.057498      23.535618   \n13  pretrained/np_model_13  np_model_13         0.055430      20.599390   \n\n    test_relmae   test_rmse  combined_relmae  combined_rmse  \\\n0      2.810304  936.227094         0.569037     407.538478   \n1      0.133273   28.749340         0.067985      25.292018   \n2      0.167335   39.322684         0.064640      24.006230   \n3      0.237883   58.603779         0.074380      27.170464   \n4      0.112888   50.128360         0.071535      28.695869   \n5      0.142858   49.697945         0.059314      23.544980   \n6      0.334624   73.170974         0.079480      30.783161   \n7      0.208862   44.170652         0.073192      26.609430   \n8      0.131529   48.569255         0.056308      22.934972   \n9      0.401554   85.650436         0.079033      29.919955   \n10     0.297688   77.034289         0.120622      40.146941   \n11     0.256770   79.088076         0.074790      30.298349   \n12     0.235159   50.387872         0.075220      27.421370   \n13     0.702958  147.524004         0.114716      48.765414   \n\n                   additional_input_names            backend  \\\n0   [dp, with_pva, polymer_concentration]  pytorch_geometric   \n1   [dp, with_pva, polymer_concentration]  pytorch_geometric   \n2   [dp, with_pva, polymer_concentration]  pytorch_geometric   \n3   [dp, with_pva, polymer_concentration]  pytorch_geometric   \n4   [dp, with_pva, polymer_concentration]  pytorch_geometric   \n5   [dp, with_pva, polymer_concentration]  pytorch_geometric   \n6   [dp, with_pva, polymer_concentration]  pytorch_geometric   \n7   [dp, with_pva, polymer_concentration]  pytorch_geometric   \n8   [dp, with_pva, polymer_concentration]  pytorch_geometric   \n9   [dp, with_pva, polymer_concentration]  pytorch_geometric   \n10  [dp, with_pva, polymer_concentration]  pytorch_geometric   \n11  [dp, with_pva, polymer_concentration]  pytorch_geometric   \n12  [dp, with_pva, polymer_concentration]  pytorch_geometric   \n13  [dp, with_pva, polymer_concentration]  pytorch_geometric   \n\n                                            fcn_model  \\\n0   {'layer': [{'args': [67, 32], 'module': 'Linea...   \n1   {'layer': [{'args': [67, 32], 'module': 'Linea...   \n2   {'layer': [{'args': [67, 32], 'module': 'Linea...   \n3   {'layer': [{'args': [67, 32], 'module': 'Linea...   \n4   {'layer': [{'args': [67, 32], 'module': 'Linea...   \n5   {'layer': [{'args': [67, 32], 'module': 'Linea...   \n6   {'layer': [{'args': [67, 32], 'module': 'Linea...   \n7   {'layer': [{'args': [67, 32], 'module': 'Linea...   \n8   {'layer': [{'args': [67, 32], 'module': 'Linea...   \n9   {'layer': [{'args': [67, 32], 'module': 'Linea...   \n10  {'layer': [{'args': [67, 16], 'module': 'Linea...   \n11  {'layer': [{'args': [67, 32], 'module': 'Linea...   \n12  {'layer': [{'args': [67, 32], 'module': 'Linea...   \n13  {'layer': [{'args': [67, 32], 'module': 'Linea...   \n\n                                           featurizer  \\\n0   [atom_symbol_hcnopsclbr_other_one_hot, atom_de...   \n1   [atom_symbol_hcnopsclbr_other_one_hot, atom_de...   \n2   [atom_symbol_hcnopsclbr_other_one_hot, atom_de...   \n3   [atom_symbol_hcnopsclbr_other_one_hot, atom_de...   \n4   [atom_symbol_hcnopsclbr_other_one_hot, atom_de...   \n5   [atom_symbol_hcnopsclbr_other_one_hot, atom_de...   \n6   [atom_symbol_hcnopsclbr_other_one_hot, atom_de...   \n7   [atom_symbol_hcnopsclbr_other_one_hot, atom_de...   \n8   [atom_symbol_hcnopsclbr_other_one_hot, atom_de...   \n9   [atom_symbol_hcnopsclbr_other_one_hot, atom_de...   \n10  [atom_symbol_hcnopsclbr_other_one_hot, atom_de...   \n11  [atom_symbol_hcnopsclbr_other_one_hot, atom_de...   \n12  [atom_symbol_hcnopsclbr_other_one_hot, atom_de...   \n13             [atom_symbol_hcnopsclbr_other_one_hot]   \n\n                         gcn_layer_sizes  gcn_output_dims  learning_rate  \\\n0   [None, None, None, None, None, None]               64          0.001   \n1   [None, None, None, None, None, None]               64          0.001   \n2   [None, None, None, None, None, None]               64          0.001   \n3   [None, None, None, None, None, None]               64          0.001   \n4   [None, None, None, None, None, None]               64          0.001   \n5   [None, None, None, None, None, None]               64          0.001   \n6   [None, None, None, None, None, None]               64          0.001   \n7   [None, None, None, None, None, None]               64          0.001   \n8   [None, None, None, None, None, None]               64          0.001   \n9   [None, None, None, None, None, None]               64          0.001   \n10  [None, None, None, None, None, None]               64          0.001   \n11                    [5, 5, 5, 5, 5, 5]               64          0.001   \n12                          [None, None]               64          0.001   \n13  [None, None, None, None, None, None]               64          0.001   \n\n                     loss_function         metrics            pooling  \\\n0   {'kwargs': {}, 'name': 'RMAE'}  [relmae, rmse]  [weight_sum, max]   \n1   {'kwargs': {}, 'name': 'RMAE'}  [relmae, rmse]  [weight_sum, max]   \n2   {'kwargs': {}, 'name': 'RMAE'}  [relmae, rmse]  [weight_sum, max]   \n3   {'kwargs': {}, 'name': 'RMAE'}  [relmae, rmse]  [weight_sum, max]   \n4   {'kwargs': {}, 'name': 'RMAE'}  [relmae, rmse]  [weight_sum, max]   \n5   {'kwargs': {}, 'name': 'RMAE'}  [relmae, rmse]  [weight_sum, max]   \n6   {'kwargs': {}, 'name': 'RMAE'}  [relmae, rmse]  [weight_sum, max]   \n7   {'kwargs': {}, 'name': 'RMAE'}  [relmae, rmse]  [weight_sum, max]   \n8   {'kwargs': {}, 'name': 'RMAE'}  [relmae, rmse]  [weight_sum, max]   \n9   {'kwargs': {}, 'name': 'RMAE'}  [relmae, rmse]  [weight_sum, max]   \n10  {'kwargs': {}, 'name': 'RMAE'}  [relmae, rmse]  [weight_sum, max]   \n11  {'kwargs': {}, 'name': 'RMAE'}  [relmae, rmse]  [weight_sum, max]   \n12  {'kwargs': {}, 'name': 'RMAE'}  [relmae, rmse]  [weight_sum, max]   \n13  {'kwargs': {}, 'name': 'RMAE'}  [relmae, rmse]  [weight_sum, max]   \n\n         smiles_column   task_names  \n0   circular_smiles_10  [z_average]  \n1   circular_smiles_10  [z_average]  \n2   circular_smiles_10  [z_average]  \n3   circular_smiles_10  [z_average]  \n4   circular_smiles_10  [z_average]  \n5   circular_smiles_10  [z_average]  \n6   circular_smiles_10  [z_average]  \n7   circular_smiles_10  [z_average]  \n8   circular_smiles_10  [z_average]  \n9   circular_smiles_10  [z_average]  \n10  circular_smiles_10  [z_average]  \n11  circular_smiles_10  [z_average]  \n12  circular_smiles_10  [z_average]  \n13  circular_smiles_10  [z_average]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>name</th>\n      <th>training_relmae</th>\n      <th>training_rmse</th>\n      <th>test_relmae</th>\n      <th>test_rmse</th>\n      <th>combined_relmae</th>\n      <th>combined_rmse</th>\n      <th>additional_input_names</th>\n      <th>backend</th>\n      <th>fcn_model</th>\n      <th>featurizer</th>\n      <th>gcn_layer_sizes</th>\n      <th>gcn_output_dims</th>\n      <th>learning_rate</th>\n      <th>loss_function</th>\n      <th>metrics</th>\n      <th>pooling</th>\n      <th>smiles_column</th>\n      <th>task_names</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>pretrained/np_model_0</td>\n      <td>np_model_0</td>\n      <td>0.046140</td>\n      <td>18.455069</td>\n      <td>2.810304</td>\n      <td>936.227094</td>\n      <td>0.569037</td>\n      <td>407.538478</td>\n      <td>[dp, with_pva, polymer_concentration]</td>\n      <td>pytorch_geometric</td>\n      <td>{'layer': [{'args': [67, 32], 'module': 'Linea...</td>\n      <td>[atom_symbol_hcnopsclbr_other_one_hot, atom_de...</td>\n      <td>[None, None, None, None, None, None]</td>\n      <td>64</td>\n      <td>0.001</td>\n      <td>{'kwargs': {}, 'name': 'RMAE'}</td>\n      <td>[relmae, rmse]</td>\n      <td>[weight_sum, max]</td>\n      <td>circular_smiles_10</td>\n      <td>[z_average]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>pretrained/np_model_1</td>\n      <td>np_model_1</td>\n      <td>0.061349</td>\n      <td>24.913739</td>\n      <td>0.133273</td>\n      <td>28.749340</td>\n      <td>0.067985</td>\n      <td>25.292018</td>\n      <td>[dp, with_pva, polymer_concentration]</td>\n      <td>pytorch_geometric</td>\n      <td>{'layer': [{'args': [67, 32], 'module': 'Linea...</td>\n      <td>[atom_symbol_hcnopsclbr_other_one_hot, atom_de...</td>\n      <td>[None, None, None, None, None, None]</td>\n      <td>64</td>\n      <td>0.001</td>\n      <td>{'kwargs': {}, 'name': 'RMAE'}</td>\n      <td>[relmae, rmse]</td>\n      <td>[weight_sum, max]</td>\n      <td>circular_smiles_10</td>\n      <td>[z_average]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>pretrained/np_model_2</td>\n      <td>np_model_2</td>\n      <td>0.054157</td>\n      <td>21.846812</td>\n      <td>0.167335</td>\n      <td>39.322684</td>\n      <td>0.064640</td>\n      <td>24.006230</td>\n      <td>[dp, with_pva, polymer_concentration]</td>\n      <td>pytorch_geometric</td>\n      <td>{'layer': [{'args': [67, 32], 'module': 'Linea...</td>\n      <td>[atom_symbol_hcnopsclbr_other_one_hot, atom_de...</td>\n      <td>[None, None, None, None, None, None]</td>\n      <td>64</td>\n      <td>0.001</td>\n      <td>{'kwargs': {}, 'name': 'RMAE'}</td>\n      <td>[relmae, rmse]</td>\n      <td>[weight_sum, max]</td>\n      <td>circular_smiles_10</td>\n      <td>[z_average]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>pretrained/np_model_3</td>\n      <td>np_model_3</td>\n      <td>0.054081</td>\n      <td>20.087497</td>\n      <td>0.237883</td>\n      <td>58.603779</td>\n      <td>0.074380</td>\n      <td>27.170464</td>\n      <td>[dp, with_pva, polymer_concentration]</td>\n      <td>pytorch_geometric</td>\n      <td>{'layer': [{'args': [67, 32], 'module': 'Linea...</td>\n      <td>[atom_symbol_hcnopsclbr_other_one_hot, atom_de...</td>\n      <td>[None, None, None, None, None, None]</td>\n      <td>64</td>\n      <td>0.001</td>\n      <td>{'kwargs': {}, 'name': 'RMAE'}</td>\n      <td>[relmae, rmse]</td>\n      <td>[weight_sum, max]</td>\n      <td>circular_smiles_10</td>\n      <td>[z_average]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>pretrained/np_model_4</td>\n      <td>np_model_4</td>\n      <td>0.067669</td>\n      <td>25.797356</td>\n      <td>0.112888</td>\n      <td>50.128360</td>\n      <td>0.071535</td>\n      <td>28.695869</td>\n      <td>[dp, with_pva, polymer_concentration]</td>\n      <td>pytorch_geometric</td>\n      <td>{'layer': [{'args': [67, 32], 'module': 'Linea...</td>\n      <td>[atom_symbol_hcnopsclbr_other_one_hot, atom_de...</td>\n      <td>[None, None, None, None, None, None]</td>\n      <td>64</td>\n      <td>0.001</td>\n      <td>{'kwargs': {}, 'name': 'RMAE'}</td>\n      <td>[relmae, rmse]</td>\n      <td>[weight_sum, max]</td>\n      <td>circular_smiles_10</td>\n      <td>[z_average]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>pretrained/np_model_5</td>\n      <td>np_model_5</td>\n      <td>0.053488</td>\n      <td>20.512828</td>\n      <td>0.142858</td>\n      <td>49.697945</td>\n      <td>0.059314</td>\n      <td>23.544980</td>\n      <td>[dp, with_pva, polymer_concentration]</td>\n      <td>pytorch_geometric</td>\n      <td>{'layer': [{'args': [67, 32], 'module': 'Linea...</td>\n      <td>[atom_symbol_hcnopsclbr_other_one_hot, atom_de...</td>\n      <td>[None, None, None, None, None, None]</td>\n      <td>64</td>\n      <td>0.001</td>\n      <td>{'kwargs': {}, 'name': 'RMAE'}</td>\n      <td>[relmae, rmse]</td>\n      <td>[weight_sum, max]</td>\n      <td>circular_smiles_10</td>\n      <td>[z_average]</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>pretrained/np_model_6</td>\n      <td>np_model_6</td>\n      <td>0.053765</td>\n      <td>22.438987</td>\n      <td>0.334624</td>\n      <td>73.170974</td>\n      <td>0.079480</td>\n      <td>30.783161</td>\n      <td>[dp, with_pva, polymer_concentration]</td>\n      <td>pytorch_geometric</td>\n      <td>{'layer': [{'args': [67, 32], 'module': 'Linea...</td>\n      <td>[atom_symbol_hcnopsclbr_other_one_hot, atom_de...</td>\n      <td>[None, None, None, None, None, None]</td>\n      <td>64</td>\n      <td>0.001</td>\n      <td>{'kwargs': {}, 'name': 'RMAE'}</td>\n      <td>[relmae, rmse]</td>\n      <td>[weight_sum, max]</td>\n      <td>circular_smiles_10</td>\n      <td>[z_average]</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>pretrained/np_model_7</td>\n      <td>np_model_7</td>\n      <td>0.058160</td>\n      <td>23.881686</td>\n      <td>0.208862</td>\n      <td>44.170652</td>\n      <td>0.073192</td>\n      <td>26.609430</td>\n      <td>[dp, with_pva, polymer_concentration]</td>\n      <td>pytorch_geometric</td>\n      <td>{'layer': [{'args': [67, 32], 'module': 'Linea...</td>\n      <td>[atom_symbol_hcnopsclbr_other_one_hot, atom_de...</td>\n      <td>[None, None, None, None, None, None]</td>\n      <td>64</td>\n      <td>0.001</td>\n      <td>{'kwargs': {}, 'name': 'RMAE'}</td>\n      <td>[relmae, rmse]</td>\n      <td>[weight_sum, max]</td>\n      <td>circular_smiles_10</td>\n      <td>[z_average]</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>pretrained/np_model_8</td>\n      <td>np_model_8</td>\n      <td>0.049371</td>\n      <td>18.893948</td>\n      <td>0.131529</td>\n      <td>48.569255</td>\n      <td>0.056308</td>\n      <td>22.934972</td>\n      <td>[dp, with_pva, polymer_concentration]</td>\n      <td>pytorch_geometric</td>\n      <td>{'layer': [{'args': [67, 32], 'module': 'Linea...</td>\n      <td>[atom_symbol_hcnopsclbr_other_one_hot, atom_de...</td>\n      <td>[None, None, None, None, None, None]</td>\n      <td>64</td>\n      <td>0.001</td>\n      <td>{'kwargs': {}, 'name': 'RMAE'}</td>\n      <td>[relmae, rmse]</td>\n      <td>[weight_sum, max]</td>\n      <td>circular_smiles_10</td>\n      <td>[z_average]</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>pretrained/np_model_9</td>\n      <td>np_model_9</td>\n      <td>0.047499</td>\n      <td>16.293620</td>\n      <td>0.401554</td>\n      <td>85.650436</td>\n      <td>0.079033</td>\n      <td>29.919955</td>\n      <td>[dp, with_pva, polymer_concentration]</td>\n      <td>pytorch_geometric</td>\n      <td>{'layer': [{'args': [67, 32], 'module': 'Linea...</td>\n      <td>[atom_symbol_hcnopsclbr_other_one_hot, atom_de...</td>\n      <td>[None, None, None, None, None, None]</td>\n      <td>64</td>\n      <td>0.001</td>\n      <td>{'kwargs': {}, 'name': 'RMAE'}</td>\n      <td>[relmae, rmse]</td>\n      <td>[weight_sum, max]</td>\n      <td>circular_smiles_10</td>\n      <td>[z_average]</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>pretrained/np_model_10</td>\n      <td>np_model_10</td>\n      <td>0.103310</td>\n      <td>34.484228</td>\n      <td>0.297688</td>\n      <td>77.034289</td>\n      <td>0.120622</td>\n      <td>40.146941</td>\n      <td>[dp, with_pva, polymer_concentration]</td>\n      <td>pytorch_geometric</td>\n      <td>{'layer': [{'args': [67, 16], 'module': 'Linea...</td>\n      <td>[atom_symbol_hcnopsclbr_other_one_hot, atom_de...</td>\n      <td>[None, None, None, None, None, None]</td>\n      <td>64</td>\n      <td>0.001</td>\n      <td>{'kwargs': {}, 'name': 'RMAE'}</td>\n      <td>[relmae, rmse]</td>\n      <td>[weight_sum, max]</td>\n      <td>circular_smiles_10</td>\n      <td>[z_average]</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>pretrained/np_model_11</td>\n      <td>np_model_11</td>\n      <td>0.058008</td>\n      <td>20.635627</td>\n      <td>0.256770</td>\n      <td>79.088076</td>\n      <td>0.074790</td>\n      <td>30.298349</td>\n      <td>[dp, with_pva, polymer_concentration]</td>\n      <td>pytorch_geometric</td>\n      <td>{'layer': [{'args': [67, 32], 'module': 'Linea...</td>\n      <td>[atom_symbol_hcnopsclbr_other_one_hot, atom_de...</td>\n      <td>[5, 5, 5, 5, 5, 5]</td>\n      <td>64</td>\n      <td>0.001</td>\n      <td>{'kwargs': {}, 'name': 'RMAE'}</td>\n      <td>[relmae, rmse]</td>\n      <td>[weight_sum, max]</td>\n      <td>circular_smiles_10</td>\n      <td>[z_average]</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>pretrained/np_model_12</td>\n      <td>np_model_12</td>\n      <td>0.057498</td>\n      <td>23.535618</td>\n      <td>0.235159</td>\n      <td>50.387872</td>\n      <td>0.075220</td>\n      <td>27.421370</td>\n      <td>[dp, with_pva, polymer_concentration]</td>\n      <td>pytorch_geometric</td>\n      <td>{'layer': [{'args': [67, 32], 'module': 'Linea...</td>\n      <td>[atom_symbol_hcnopsclbr_other_one_hot, atom_de...</td>\n      <td>[None, None]</td>\n      <td>64</td>\n      <td>0.001</td>\n      <td>{'kwargs': {}, 'name': 'RMAE'}</td>\n      <td>[relmae, rmse]</td>\n      <td>[weight_sum, max]</td>\n      <td>circular_smiles_10</td>\n      <td>[z_average]</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>pretrained/np_model_13</td>\n      <td>np_model_13</td>\n      <td>0.055430</td>\n      <td>20.599390</td>\n      <td>0.702958</td>\n      <td>147.524004</td>\n      <td>0.114716</td>\n      <td>48.765414</td>\n      <td>[dp, with_pva, polymer_concentration]</td>\n      <td>pytorch_geometric</td>\n      <td>{'layer': [{'args': [67, 32], 'module': 'Linea...</td>\n      <td>[atom_symbol_hcnopsclbr_other_one_hot]</td>\n      <td>[None, None, None, None, None, None]</td>\n      <td>64</td>\n      <td>0.001</td>\n      <td>{'kwargs': {}, 'name': 'RMAE'}</td>\n      <td>[relmae, rmse]</td>\n      <td>[weight_sum, max]</td>\n      <td>circular_smiles_10</td>\n      <td>[z_average]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "source_folder=\"pretrained/\"\n",
    "print(sorted(os.listdir(source_folder),key=lambda f: int(f.rsplit(\"_\",maxsplit=1)[1])))\n",
    "eval_data=[]\n",
    "for model_path in sorted(os.listdir(source_folder),\n",
    "                         #reverse=True,\n",
    "                         key=lambda f: int(f.rsplit(\"_\",maxsplit=1)[1])\n",
    "                         ):\n",
    "    ev = evaluate(os.path.join(source_folder,model_path))\n",
    "\n",
    "    eval_data.append(ev)\n",
    "\n",
    "eval_df=pd.DataFrame(eval_data)\n",
    "display(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#model.save_last_prediction_tensor=True\n",
    "#model.save_last_input_data=True\n",
    "\n",
    "#test_df.loc[np.isnan(test_df[\"dp\"]),\"dp\"]=400\n",
    "#if \"level_0\" in test_df.columns:\n",
    "#    test_df.drop(\"level_0\",axis=1,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#mask = test_df[\"dp\"]== test_df[\"dp\"]-1\n",
    "#n_mask = test_df[\"dp\"] == (test_df[test_df[\"rel_error\"] == test_df[\"rel_error\"].max()][\"dp\"]).unique()[0]\n",
    "#mask = n_mask\n",
    "\n",
    "#n_mask = test_df[\"dp\"] == (test_df[test_df[\"rel_error\"] == test_df[~mask][\"rel_error\"].max()][\"dp\"]).unique()[0]\n",
    "#mask = mask | n_mask \n",
    "\n",
    "#raise Exception(\"remove me\")\n",
    "\n",
    "#python main.py --train=\"__last\" --model_path=\"pretrained/np_model_11\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}